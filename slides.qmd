---
title: "PROMPI: a DiRAC RSE success story"
subtitle: "DiRAC Science Day 2024"
date: "2024-12-12"
date-format: "dddd DD MMMM YYYY"
format:
  clean-revealjs:
    embed-resources: true

authors:
 - name: Miren Radia
   role: Research Software Engineer
   affiliations:
     - name: University of Cambridge

---
# Introduction {background-color="#40666e"}

## The team {.smaller}

:::: {.columns}

::: {.column width="30%"}
![](media/raphael.jpg){width="80%"}

**Raphael Hirschi**</br>
[Professor of Stellar Hydrodynamics and Nuclear Astrophysics]{.alert}</br>
*Keele University*
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
![](media/vishnu.jpg){width="80%"}

**Vishnu Varma**</br>
[Research Associate In Theoretical Stellar Astrophysics]{.alert}</br>
*Keele University*
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
![](media/miren.jpg){width="80%"}

**Miren Radia**</br>
[Research Software Engineer]{.alert}</br>
*University of Cambridge*
:::

::::

#### Others

* Federico Rizzuti, former PhD Student, *Keele University*
* Caitlyn Chambers, new PhD student, *Keele University*

## PROMPI {.smaller}
### What does the code do?

:::: {.columns}

::: {.column width="50%"}
* PROMPI is a fluid dynamics code that is used to simulate complex hydrodynamic
  processes within stars.
* Numerical methods:
  * Finite volume
  * Eulerian
  * Piecewise Parabolic Method (PPM) hydrodynamics scheme
* Physics:
  * Fully compressible fluids
  * Nuclear burning
  * Convection/turbulence
* Code:
  * Fortran
  * Parallelised with domain decomposition distributed with MPI
:::

::: {.column width="50%"}
<!-- {{< video vmag_vhrez.mp4 width=540 height=540 >}} -->
![Evolution of $|\mathbf{v}|$ for a $1024^3$ simulation of the Carbon-burning
shell](media/vmag_vhrez.mp4)
:::

::::

## Previous RSE work
### What improvements had already been made to the code?

Over several DiRAC RSE projects, the code has been enhanced and modernized in
several different ways:

* Acceleration on Nvidia GPUs using OpenACC
* Fortran 77 → Modern free-form Fortran
* Object-oriented design (Fortran 2003)
* Legacy include statements and common blocks → Modules
* Custom Makefile build system → CMake
* Custom binary I/O format → HDF5
* Regression tests and GitLab CI pipeline to run them

# This project {background-color="#40666e"}

## Aims
### What still needed to be done for the new code to be research-ready?

Despite the enhancements, there was still work that needed to be done before the
group felt they could switch over:

1. Consistency between the results on GPU and CPU.
1. Optimal performance on the DiRAC systems the group uses (COSMA8 and Tursa).
1. Porting and testing of physics modules and initial conditions to simulate
   specific scenarios from the old version of the code.
1. Poor scaling on GPUs beyond a single GPU.

## Work summary
### What improvements were made to the code?

During the project, changes I made include:

* Improvements and updates to the CMake build system.
* Dependency software stack creation on Tursa and greenHPC (Keele local system).
* Refactoring, updating and adding to the test and CI frameworks.
* Fixing and refactoring the analysis/plotting Python scripts.
* [Significant refactoring of the MPI communication.]{.alert}
* Fixing the HDF5 checkpoint and restart consistency.
* [Benchmarking and scaling analysis.]{.alert}

# Improving MPI communication {background-color="#40666e"}

## The problem {.smaller}
### What was causing such poor performance on GPUs?

Previously the code used:

* Nvidia managed memory extension to OpenACC:
  * The runtime automatically migrates data between host (CPU) and device
    (GPU) as required.
* MPI Derived Datatypes:
  * `MPI_Type_vector` to simplify halo/ghost cell exchange since this data is
     non-contiguous in memory, albeit regularly spaced.
* Effectively blocking MPI calls
  * `MPI_Wait` was called after every `MPI_Irecv`.

This combination meant lots of small host-device data migrations → bad for
performance:

* For a $512^3$ test simulation running on 8 Tursa Nvidia A100s (2 nodes), > 90%
  of the walltime was spent in communication.


## The solution

# Benefits {background-color="#40666e"}

## Scaling

## Other benefits

# Any questions?